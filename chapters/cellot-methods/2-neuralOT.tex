% TODO: major pass over all of this
% OT to dual formulaiton uses general cost, then introduce Euclidiean costs at brenier's
\section{Optimal Transport}
Optimal transport (OT) is a rich branch of mathematics that describe how to manipulate and transform probability distributions.
It plays a pair of roles:
(i) inducing a mathematically well-characterized distance measure between distributions
and (ii) providing a geometry-based approach to realize couplings between two probability distributions.

Let $\mu$ and $\nu$ be probability measures over spaces $\mathcal{X}$ and $\mathcal{Y}$ respectively,
and $T : \mathcal{X} \mapsto \mathcal{Y}$.

\paragraph{Push-forward operator}
As optimal transport describes how to transform probability measures,
it is convienent to introduce notation to help describe these transformations.
While $T$ is a function that maps individual points from $\mathcal{X}$ to $\mathcal{Y}$,
the push-forward operator, $\sharp$, allows $T_\sharp$ to
map a \emph{whole} probability measure on $\mathcal{X}$ to a probability measure on $\mathcal{Y}$.
Intuitively, the push-forward applies the function $T$ to all of the elementary mass particles in the measure $\mu$,
thereby, $\sharp$ uses $T$ to "push" the measure $\mu$ from $\mathcal{X}$ onto some measure on $\mathcal{Y}$.

While the push-forward operator can be defined over arbitraty probability measures, % TODO: cite
for simplicity, we consider here the discrete case.
Given a discrete distribution $\alpha$ over $\mathcal{X}$, $\alpha = \sum_i \mathbf{a}_i \delta_{x_i}$
where $\sum_i \mathbf{a}_i = 1$ and $\delta_a$ is the dirac delta concentrated at some location $x \in \mathcal{X}$,
the push-forward operator is defined as 
\begin{equation}
  \label{eq:def-pushfwd}
  T_\sharp\alpha := \sum_i \mathbf{a}_i \delta_{T(x_i)}
\end{equation}
$T_\sharp\alpha$ now describes a discrete probability distribution over $\mathcal{Y}$.

\subsection{Monge Maps}
% Let $\mu$ and $\nu$ be two measures in $\mathbb{R}^d$ and a cost function $c: \mathcal{X} \times \mathcal{Y} \mapsto \mathbb{R}$.
The optimal transport problem as formulated by Monge identifies the function
$T^\star$ that transforms $\mu$ into $\nu$, i.e. $T_\sharp \mu = \nu$,
by moving mass as efficeiently as possible according to the cost $c$.
It is defined as
\begin{equation}
  % T^\star = {\arg \inf}_{T : T_\sharp \mu = \nu} \int_{\mathcal{X}} c(x, T(x))d\mu(x)
  T^\star = {\arg \inf}_{T : T_\sharp \mu = \nu} \mathbb{E}_\mu c(x, T(x))
  \label{eq:monge}
\end{equation}

\paragraph{Monge maps for discrete distributions}
Consider two uniform discrete probability distributions, $\mu$ and $\nu$, of size $n$,
$\mu = \frac{1}{n} \sum_{i=1}^n \delta_{x_i}$
and $\nu = \frac{1}{n} \sum_{j=1}^n \delta_{y_j}$.
In this special case, the Monge problem is an assignment problem: % TODO: cite smth
\begin{equation}
  \min_{\sigma \in \text{Perm}(n)} \frac{1}{n} \sum_{i=1}^{n} \mathbf{C}_{i,\sigma(i)}
\end{equation}
where $\mathbf{C}_{i,j} := c(x_i, y_j)$ is the cost to assign $x_i$ to $y_j$,
that is, $T^\star(x_i) = y_{\sigma(i)}$.
$\mathbf{C}$ can be considered as the weighted adjacency matrix of the bipartite graph that connects elements of $\mu$ to elements of $\nu$.
Here, the optimal transport problem and bipartite matching problem are equivalent.
% TODO: link to bipartite matching in SCIM section

\paragraph{Limitations}
The \ref{eq:monge} Monge formulation comes with its own set of limitations and challenges.
Firstly, its optimization is a complex non-convex problem.
Furthermore, its reliance on a \emph{deterministic} transport function $T$ restricts the set of problems it can be applied to.
Consider for instance that $\mu$ and $\nu$ are uniform discrete probability measures over $n$ and $m$ items, respectively.
If $n < m$, then there cannot exist a Monge solution as there is no way for a $T$ to assign a source $x_i$ to more than one target $y_j$.
Similarly, solutions for non-uniform discrete measures are not guaranteed to exists,
due to limitations stemming from the determinism of the Monge formulation.

\subsection{Kantorovich relaxation}  % eqn 2.24 in computational OT
\citet{kantorovich1942transfer} provides a formulation to address these shortcomings with a relaxation to the
Monge problem that allows for non-deterministic transportation by "splitting" mass.
The \citet{kantorovich1942transfer} optimal transport formulation is 
\begin{equation}\label{eq:ot-kantorovich}
W(\mu, \nu)= \inf _{\gamma \in \Gamma(\mu, \nu)} \mathbb{E}_{(X, Y) \sim \gamma}c(x, y)
\end{equation}
where the polytope $\Gamma(\mu, \nu)$ is the set of all joint distributions (or couplings) $\gamma$ between $\mu$ and $\nu$.
The optimal transport plan $\gamma^\star$ that minimizes the Wasserstein distance thus corresponds to the coupling between two probability distributions minimizing the overall transportation cost.
Computing optimal transport distances in \eqref{eq:ot-kantorovich} involves solving a linear program,
and thus their computational cost is prohibitive for large-scale machine learning problems.
Regularizing objective \eqref{eq:ot-kantorovich} with an entropy term results in significantly more efficient optimization
\citep{cuturi2013sinkhorn} and differentiability w.r.t. its inputs, and thus commonly used as a loss function in machine learning applications.

% \begin{equation}\label{eq:ot-reg}
% W_{2}^{2, \varepsilon}(\mu, \nu)=\inf _{\gamma \in \Gamma(\mu, \nu)} \int\|x-y\|^{2} d \gamma(x, y) - \varepsilon H(\gamma),
% \end{equation}
% with entropy $H(\gamma) = -\sum_{ij} \gamma_{ij} (\log \gamma_{ij} - 1)$ and parameter $\varepsilon$ controlling the strength of the regularization. $W_{2}^\varepsilon$ is further differentiable w.r.t. its inputs and thus 

\paragraph{Kantorovich Dual}
Problem~\eqref{eq:ot-kantorovich} denotes the primal formulation for the Wasserstein distance. The corresponding dual introduced by \citeauthor{kantorovich1942transfer} in \citeyear{kantorovich1942transfer} is a constrained concave maximization problem defined as
\begin{equation} \label{eq:ot-dual}
    W(\mu, \nu)=\sup _{(f, g) \in \Phi_{c}} \mathbb{E}_{\mu}[f(x)]+\mathbb{E}_{\nu}[g(y)],
\end{equation}
where the set of admissible potentials is $\phi_c := \{(f, g) \in L^{1}(\mu) \times L^{1}(\nu): f(x)+g(y) \leq c(x, y)$, $\forall(x, y) d\mu \otimes d\nu \text{ a.e.}\}$ \citep[Theorem 1.3]{villani2021topics}.
% TODO: maybe add intuition here

\subsection{Brenier's Theorem}
% TODO: minor describe
Of particular interest of optimal transport is when $\mathcal{X} = \mathcal{Y} = \mathbb{R}^d$ and $c(x, y) = \frac{1}{2} \| x - y \|^2$.
Thanks to a series of results often attributed to Brenier \cite{brenier}\ldots % TODO: minor add authors
some convienent simplifications can be made to the Kantorovich dual formulation \ref{eq:ot-dual}.

Within this setting, \citet[Theorem 2.9]{villani2021topics} simplifies the dual problem \eqref{eq:ot-dual} over the pair of functions $(f, g)$ to
an optimization over a single $f$ and its convex conjugate, $f^*$, where $ f^*(y) = \sup_x \left<x,y\right> - f(x)$.
\begin{equation} \label{eq:ot-dual-brenier}
    W(\mu, \nu)= \frac{1}{2}\mathbb{E}\left[\|x\|_{2}^{2}+\|y\|_{2}^{2}\right]-\inf _{f \in \Tilde{\Phi}} \mathbb{E}_{\mu}[f(X)]+\mathbb{E}_{\nu}\left[f^{*}(Y)\right],
\end{equation}
where $\Tilde{\Phi}$ is the set of all convex functions in $L^1(d\mu) \times L^1(d\nu)$,
% where $L^{1}(\mu) := \{f \text{ is measurable } \& \int f d\mu<\infty\}$,
$L^{1}(\mu)$ is the set of measurable $f$ such that $\mathbb{E}_\mu f(x) < \infty$.
Furthermore, the optimal transport plan corresponds to \emph{gradient} of the convex conjugate, $\gamma^\star = \nabla f^\star$.
\citet[Theorem 2.9]{villani2021topics} then proves the existence of an optimal pair $(f, f^*)$ of lower semi-continuous proper conjugate convex functions on $\mathbb{R}^n$ minimizing \eqref{eq:ot-dual}.

\subsection{Optimal transport applications in single-cell biology}
Following pioneering work by \citet{schiebinger2019optimal}, numerous problems in single-cell biology have recently been approached using optimal transport.
These problems include mapping cells across perturbations, time points, experimental batches, as well as reconstructing spatial structure from gene expression.
In contrast to previous approaches \citep{schiebinger2019optimal, lavenant2021towards},
we seek to learn and thus parameterize the optimal transport map $T$ to allow forecasting and predictions on \emph{unseen} cell populations,
i.e., in the out-of-sample setting.
Existing methods addressed proposed neural network-based OT models that directly parameterize $T$ \citep{jacob2018w2gan, yang2018scalable, prasad2020optimal}.
This, however, has been shown to yield an unstable and difficult-to-solve optimization problem~\citep[Table 1]{makkuva2020optimal}.

% TODO: minor rephrase to connect
\section{Neural Optimal Transport}
The computed couplings made by traditional OT appraoches rely on observations of the source and target distributions.
This reliance represents a strong limitation in terms of their viablitiy in \emph{prediction} tasks.
For example, in the context of single-cell perturbation prediction,
these approaches would require that some set of perturbed states are always present,
thus are limited in terms of their applications.

Neural OT methods address this limitation by constructing frameworks to parameterize and learn the transport plan $T$ using neural networks.
While the formulation described in Equation $\ref{eq:ot-dual}$ is valid for arbitrary cost functions,
imposing its constraint that the coupling $g(x) + f(y) \leq c(x, y)$ must be enforced everywhere during optimization is generally quite difficult.
A popular approach is to instead build upon the celebrated results by \citet{knott1984optimal} and \citet{brenier1991polar}
and operate from the formulation described in Equation $\ref{eq:ot-dual-brenier}$.
The optimization scheme within this approach is easier to implement, however, it does require
the adoption of an Euclidean cost function and a framework to parameterize the convex function $f$.
Ultimately, the parameterization of the optimal transport plan $T$ is achieved using the gradient of the convex conjugate of $f$, $\nabla f^*$.

\paragraph{Input Convex Neural Networks}
%\subsection{Convex neural networks}
% Convex neural network architectures are neural networks $f(x; \theta)$ with specific constraints on the architecture and parameters $\theta$, such that the output is a convex function of some elements of the input $x$~\citep{amos2017input}.
Convex spaces such as $\Tilde{\Phi}$ in \eqref{eq:ot-dual-brenier}, can be parameterized utilizing neural networks which are convex w.r.t. to their inputs. One such parameterization approach are input convex neural networks (ICNNs) introduced by \citet{amos2017input}.
ICNNs are based on fully-connected feed-forward networks that ensure convexity by placing constraints on their parameters.
An ICNN with parameters $\theta = \{b_i, W^z_i, W^x_i\}$ represents a convex function $f(x; \theta)$ and, for a layer $i = {0 \ldots L-1}$, is defined as
\begin{equation}
    h_{i+1} = \sigma_i(W^x_ix + W^z_i h_i + b_i)  \text { and } f(x; \theta) = h_L,
\end{equation}
where activation functions $\sigma_i$ are convex and non-decreasing, and elements of all $W^z_i$ are constrained to be nonnegative.
Despite these constraints, ICNNs are able to parameterize a rich class of convex functions.
In particular, \citet{chen2018optimal} provide a theoretical analysis that any convex function over a convex domain can be approximated in sup norm by an ICNN.
\citet{huang2021convex} further extend ICNNs from fully-connected feed-forward neural networks to convolutional neural architectures.

\subsection{Parametrization of the optimal transport map}
% TODO: fix the x and y in this section. Always f(y) and g(x) 
In this chapter, we follow the approach taken by \citeauthor{makkuva2020optimal}.
This approach builds off of a previous approach described by \citet{taghvaei20192} who consider solving \eqref{eq:ot-dual-brenier} by parameterizing $f$ with an ICNN and directly solving, at each update step, an optimzation problem to compute the convex conjugate $f^*$.
As these updates are costly, \citetmain{makkuva2020optimal} relax the strict dependency between $f$ and $f^*$
by approximating $f^*$ with another ICNN $g$.
This results in a min-max optimization problem over a pair of competing convex neural networks:
\begin{equation} \label{eq:ot-makkuva}
  W_{2}^2(\mu, \nu)=\sup _{\substack{f \in \Tilde{\Phi}}} \inf _{g \in \Tilde{\Phi}}  \underbrace{\frac{1}{2}\mathbb{E}\left[\|x\|_{2}^{2}+\|y\|_{2}^{2}\right]}_{\mathcal{C}_{\mu, \nu}} - \underbrace{\mathbb{E}_{\mu}[f(x)]-\mathbb{E}_{\nu}[\langle y, \nabla g(y)\rangle-f(\nabla g(y))]}_{\mathcal{V}_{\mu, \nu}(f, g)}.
\end{equation}
Where we have utilized the fact that
\begin{equation}
  \mathbb{E}_\mu \left[f^*(x)\right] = \max_{g \in \Tilde{\phi}} \mathbb{E}_\mu \left[ \left<x, \nabla g(x)\right> - f(\nabla g(x))\right]
  \label{eq:ot-makkuva-hint}
\end{equation}
and 
$\left<x, \nabla g(x)\right> - f(\nabla g(x)) \leq f^*(x)$ for all functions $g$ with equality when $g=f^*$.
Finally, the parameterization of the optimal transport plan $T^\star$ is achieved by solving the min-max problem:
\begin{equation}
  (g^\star_\theta, f^\star_\phi) \leftarrow \arg \max_\phi \min_\theta \mathcal{C}_{\mu,\nu} - \mathcal{V}_{\mu, \nu}(g_\theta, f_\phi)
  \label{eq:ot-makkuva-optim}
\end{equation}
where $T^\star = \nabla g^\star_\theta$ and $\theta$ and $\phi$ are the parameters the ICNNs.

% In order to learn the resulting optimal transport, i.e., the solution of the minimization problem in \eqref{eq:ot-makkuva}, \citetmain{makkuva2020optimal} parameterize both dual variables $f$ and $g$ using ICNNs \citep{amos2017input} and yields a transport plan with the graident of $g$.
% The resulting approximate Wasserstein distance is thus defined as
% \begin{equation} \label{eq:cellot-optim}
%     \hat{W}_2^2(\mu, \nu) = \sup _{\phi} \inf_{\theta}  \mathcal{C}_{\mu, \nu} - \mathcal{V}_{\mu, \nu}(f_{\phi}, g_{\theta}),
% \end{equation}
% where $\phi$ and $\theta$ are the parameters of the ICNNs underlying $f$ and $g$ respectively.


\paragraph{Other approaches}
Other appraoches, for example taken by \citet{huang2021convex} introduce a novel, OT-inspired parameterization of normalizing flows utilizing ICNNs.
\citet{korotin2021neural} provide a detailed comparison of the current state of neural optimal transport solvers.
Furthermore, convex neural architectures have been utilized to parameterize Wasserstein gradient flows
\citep{bunne2022proximal, alvarez2021optimizing, mokrov2021large} as well as barycenters \citep{fan2020scalable}.

